{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.408310985Z\",\"message\":{\"role\":\"assistant\",\"content\":\" I\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.427340534Z\",\"message\":{\"role\":\"assistant\",\"content\":\" am\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.443748862Z\",\"message\":{\"role\":\"assistant\",\"content\":\" an\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.458293776Z\",\"message\":{\"role\":\"assistant\",\"content\":\" AI\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.472687789Z\",\"message\":{\"role\":\"assistant\",\"content\":\" and\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.487121202Z\",\"message\":{\"role\":\"assistant\",\"content\":\" do\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.50100081Z\",\"message\":{\"role\":\"assistant\",\"content\":\" not\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.515648525Z\",\"message\":{\"role\":\"assistant\",\"content\":\" have\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.529890536Z\",\"message\":{\"role\":\"assistant\",\"content\":\" feelings\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.544191948Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.558531661Z\",\"message\":{\"role\":\"assistant\",\"content\":\" but\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.572945573Z\",\"message\":{\"role\":\"assistant\",\"content\":\" I\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.587244585Z\",\"message\":{\"role\":\"assistant\",\"content\":\" am\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.601442996Z\",\"message\":{\"role\":\"assistant\",\"content\":\" functioning\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.615205504Z\",\"message\":{\"role\":\"assistant\",\"content\":\" well\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.629136713Z\",\"message\":{\"role\":\"assistant\",\"content\":\".\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.642998222Z\",\"message\":{\"role\":\"assistant\",\"content\":\" How\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.656988931Z\",\"message\":{\"role\":\"assistant\",\"content\":\" can\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.67089614Z\",\"message\":{\"role\":\"assistant\",\"content\":\" I\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.68491965Z\",\"message\":{\"role\":\"assistant\",\"content\":\" assist\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.698794858Z\",\"message\":{\"role\":\"assistant\",\"content\":\" you\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.712732667Z\",\"message\":{\"role\":\"assistant\",\"content\":\" today\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.726714977Z\",\"message\":{\"role\":\"assistant\",\"content\":\"?\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.740802887Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\n\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.754868397Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\n\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.769018108Z\",\"message\":{\"role\":\"assistant\",\"content\":\"Your\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.782926217Z\",\"message\":{\"role\":\"assistant\",\"content\":\" assistant\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.796996427Z\",\"message\":{\"role\":\"assistant\",\"content\":\" is\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.810464132Z\",\"message\":{\"role\":\"assistant\",\"content\":\" ready\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.824605843Z\",\"message\":{\"role\":\"assistant\",\"content\":\" to\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.83836685Z\",\"message\":{\"role\":\"assistant\",\"content\":\" help\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.85232556Z\",\"message\":{\"role\":\"assistant\",\"content\":\" you\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.865785365Z\",\"message\":{\"role\":\"assistant\",\"content\":\" with\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.880053277Z\",\"message\":{\"role\":\"assistant\",\"content\":\" your\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.894012886Z\",\"message\":{\"role\":\"assistant\",\"content\":\" questions\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.908003495Z\",\"message\":{\"role\":\"assistant\",\"content\":\" or\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.921961305Z\",\"message\":{\"role\":\"assistant\",\"content\":\" tasks\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.936029115Z\",\"message\":{\"role\":\"assistant\",\"content\":\".\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.949552921Z\",\"message\":{\"role\":\"assistant\",\"content\":\" If\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.963613731Z\",\"message\":{\"role\":\"assistant\",\"content\":\" you\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.977302938Z\",\"message\":{\"role\":\"assistant\",\"content\":\" need\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:23.991192846Z\",\"message\":{\"role\":\"assistant\",\"content\":\" assistance\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.004716552Z\",\"message\":{\"role\":\"assistant\",\"content\":\" with\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.018749362Z\",\"message\":{\"role\":\"assistant\",\"content\":\" something\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.032425269Z\",\"message\":{\"role\":\"assistant\",\"content\":\" specific\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.046363278Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.059906084Z\",\"message\":{\"role\":\"assistant\",\"content\":\" feel\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.073943294Z\",\"message\":{\"role\":\"assistant\",\"content\":\" free\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.087650801Z\",\"message\":{\"role\":\"assistant\",\"content\":\" to\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.10159411Z\",\"message\":{\"role\":\"assistant\",\"content\":\" ask\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.115145316Z\",\"message\":{\"role\":\"assistant\",\"content\":\"!\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.129221427Z\",\"message\":{\"role\":\"assistant\",\"content\":\" For\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.142930134Z\",\"message\":{\"role\":\"assistant\",\"content\":\" instance\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.156910643Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.17056905Z\",\"message\":{\"role\":\"assistant\",\"content\":\" you\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.184715261Z\",\"message\":{\"role\":\"assistant\",\"content\":\" might\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.198504969Z\",\"message\":{\"role\":\"assistant\",\"content\":\" be\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.212566279Z\",\"message\":{\"role\":\"assistant\",\"content\":\" looking\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.226169285Z\",\"message\":{\"role\":\"assistant\",\"content\":\" for\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.240320896Z\",\"message\":{\"role\":\"assistant\",\"content\":\" information\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.254138004Z\",\"message\":{\"role\":\"assistant\",\"content\":\" on\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.268156214Z\",\"message\":{\"role\":\"assistant\",\"content\":\" a\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.281831321Z\",\"message\":{\"role\":\"assistant\",\"content\":\" certain\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.296020332Z\",\"message\":{\"role\":\"assistant\",\"content\":\" topic\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.309930041Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.32395475Z\",\"message\":{\"role\":\"assistant\",\"content\":\" need\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.337618257Z\",\"message\":{\"role\":\"assistant\",\"content\":\" help\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.351776068Z\",\"message\":{\"role\":\"assistant\",\"content\":\" troubles\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.365604376Z\",\"message\":{\"role\":\"assistant\",\"content\":\"h\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.379624286Z\",\"message\":{\"role\":\"assistant\",\"content\":\"oot\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.393260993Z\",\"message\":{\"role\":\"assistant\",\"content\":\"ing\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.407554605Z\",\"message\":{\"role\":\"assistant\",\"content\":\" a\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.421388613Z\",\"message\":{\"role\":\"assistant\",\"content\":\" problem\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.435423923Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.449068429Z\",\"message\":{\"role\":\"assistant\",\"content\":\" or\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.463300041Z\",\"message\":{\"role\":\"assistant\",\"content\":\" just\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.477119349Z\",\"message\":{\"role\":\"assistant\",\"content\":\" want\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.491226759Z\",\"message\":{\"role\":\"assistant\",\"content\":\" to\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.504898466Z\",\"message\":{\"role\":\"assistant\",\"content\":\" chat\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.518988477Z\",\"message\":{\"role\":\"assistant\",\"content\":\" about\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.532654084Z\",\"message\":{\"role\":\"assistant\",\"content\":\" a\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.546685593Z\",\"message\":{\"role\":\"assistant\",\"content\":\" variety\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.5603306Z\",\"message\":{\"role\":\"assistant\",\"content\":\" of\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.57436431Z\",\"message\":{\"role\":\"assistant\",\"content\":\" topics\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.588002917Z\",\"message\":{\"role\":\"assistant\",\"content\":\".\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.602073327Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Whatever\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.615767934Z\",\"message\":{\"role\":\"assistant\",\"content\":\" it\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.629846144Z\",\"message\":{\"role\":\"assistant\",\"content\":\" is\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.643522951Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.657589161Z\",\"message\":{\"role\":\"assistant\",\"content\":\" I\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.671282468Z\",\"message\":{\"role\":\"assistant\",\"content\":\" am\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.685367679Z\",\"message\":{\"role\":\"assistant\",\"content\":\" here\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.699111786Z\",\"message\":{\"role\":\"assistant\",\"content\":\" to\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.712946294Z\",\"message\":{\"role\":\"assistant\",\"content\":\" help\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.726760103Z\",\"message\":{\"role\":\"assistant\",\"content\":\"!\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.740627811Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Let\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.75453442Z\",\"message\":{\"role\":\"assistant\",\"content\":\" me\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.768386628Z\",\"message\":{\"role\":\"assistant\",\"content\":\" know\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.782251137Z\",\"message\":{\"role\":\"assistant\",\"content\":\" what\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.796060445Z\",\"message\":{\"role\":\"assistant\",\"content\":\" you\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.809929853Z\",\"message\":{\"role\":\"assistant\",\"content\":\" would\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.823805462Z\",\"message\":{\"role\":\"assistant\",\"content\":\" like\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.83765337Z\",\"message\":{\"role\":\"assistant\",\"content\":\" to\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.851523579Z\",\"message\":{\"role\":\"assistant\",\"content\":\" discuss\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.865433788Z\",\"message\":{\"role\":\"assistant\",\"content\":\" or\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.879159695Z\",\"message\":{\"role\":\"assistant\",\"content\":\" how\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.892855102Z\",\"message\":{\"role\":\"assistant\",\"content\":\" I\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.90658551Z\",\"message\":{\"role\":\"assistant\",\"content\":\" can\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.920283917Z\",\"message\":{\"role\":\"assistant\",\"content\":\" assist\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.934039925Z\",\"message\":{\"role\":\"assistant\",\"content\":\" you\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.947827932Z\",\"message\":{\"role\":\"assistant\",\"content\":\" today\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.961745641Z\",\"message\":{\"role\":\"assistant\",\"content\":\".\"},\"done\":false}\n",
      "{\"model\":\"mistral\",\"created_at\":\"2025-02-19T20:55:24.97561655Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":1631591468,\"load_duration\":10991786,\"prompt_eval_count\":11,\"prompt_eval_duration\":21000000,\"eval_count\":113,\"eval_duration\":1597000000}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"https://ollama-proxy.cent-su.org/api/chat\",\n",
    "    json={\"model\": \"mistral\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]}\n",
    ")\n",
    "\n",
    "print(response.text)  # Print raw response before parsing JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, I can! The history of cookies dates back thousands of years. The concept of small, portable, baked snacks can be traced to ancient civilizations such as Egypt, Greece, and Rome. However, the term \"cookie\" as we know it today comes from the Dutch and German words for little cakes, \"koekje\" and \"kek,\" respectively.\n",
      "\n",
      "The earliest form of cookies were probably biscuits - simple flatbreads baked and hardened to keep for long journeys or military campaigns. These were often made with oats rather than flour.\n",
      "\n",
      "In medieval Europe, sweeter cookies were popularized, often containing honey, spices, and fruits, as sugar became more affordable. One of the earliest recorded cookie-like treats was lebkuchen, a spiced gingerbread that originated in Germany during the Middle Ages.\n",
      "\n",
      "The first printed recipe for cookies appeared in \"Accomodation for the Festival of Cookery,\" a manuscript written by Thomas Dawson and published in 1584. The book contained a variety of recipes, including one for \"Cookies\" made with almonds, rosewater, and sugar.\n",
      "\n",
      "As sugar became more widespread and affordable in the Western world during the 17th and 18th centuries, cookies became increasingly sweet and diverse in flavors. By the late 19th century, mass-produced cookies were becoming popular in America, with companies such as Nabisco and Keebler producing popular brands like Oreos and Animal Crackers.\n",
      "\n",
      "Today, the cookie industry is vast and diverse, with countless variations in flavor, size, texture, and ingredients. Cookies remain a beloved snack across the globe and continue to evolve with modern trends and dietary needs.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json  # Import the correct JSON module\n",
    "\n",
    "response = requests.post(\n",
    "    \"https://ollama-proxy.cent-su.org/api/chat\",\n",
    "    json={\"model\": \"mistral\", \"messages\": [{\"role\": \"user\", \"content\": \"Can you tell me about the history of cookies?\"}]},\n",
    "    stream=True  # Enable streaming response\n",
    ")\n",
    "\n",
    "full_response = \"\"  # Store the full message\n",
    "\n",
    "# Process line-by-line streaming JSON\n",
    "for line in response.iter_lines():\n",
    "    if line:  # Ignore empty lines\n",
    "        json_obj = json.loads(line.decode(\"utf-8\"))  # Correctly decode and parse JSON\n",
    "        if \"message\" in json_obj and \"content\" in json_obj[\"message\"]:\n",
    "            full_response += json_obj[\"message\"][\"content\"]  # Append message content\n",
    "\n",
    "print(full_response)  # Output the full AI response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  Here is a simple Python function that takes a string as input and returns the same string converted to lowercase:\n",
      "\n",
      "```python\n",
      "def to_lowercase(text):\n",
      "    return text.lower()\n",
      "```\n",
      "\n",
      "You can call this function by passing your text as an argument like so:\n",
      "\n",
      "```python\n",
      "my_text = \"Hello, World!\"\n",
      "result = to_lowercase(my_text)\n",
      "print(result)  # Output: \"hello, world!\"\n",
      "```\n",
      "\n",
      "This function uses the built-in `str.lower()` method in Python, which converts all uppercase letters to their lowercase equivalents in the given string. The method returns the modified string so you can assign it to a variable or use it directly as needed.\n",
      "Ending chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Initialize message history\n",
    "conversation_history = []\n",
    "\n",
    "# Choose model\n",
    "model_name = \"mistral\"  # You can change this to \"llama2\" or another model\n",
    "\n",
    "# Start chat loop\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    # Exit condition\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Ending chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Add user message to conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Send request to Ollama proxy\n",
    "    response = requests.post(\n",
    "        \"https://ollama-proxy.cent-su.org/api/chat\",\n",
    "        json={\"model\": model_name, \"messages\": conversation_history},\n",
    "        stream=True  # Enable streaming response\n",
    "    )\n",
    "\n",
    "    # Process response\n",
    "    full_response = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            json_obj = json.loads(line.decode(\"utf-8\"))\n",
    "            if \"message\" in json_obj and \"content\" in json_obj[\"message\"]:\n",
    "                full_response += json_obj[\"message\"][\"content\"]\n",
    "\n",
    "    # Print assistant's response\n",
    "    print(\"AI:\", full_response)\n",
    "\n",
    "    # Add assistant response to conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": full_response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'write me code that tells me what the weather is'},\n",
       " {'role': 'assistant',\n",
       "  'content': ' To get the current weather, you would typically use APIs provided by weather services like OpenWeatherMap or Weatherbit. Here\\'s an example using Python and requests library for accessing the OpenWeatherMap API:\\n\\n```python\\nimport requests\\n\\napi_key = \"your_openweathermap_api_key\"  # Replace with your actual API key\\nbase_url = \"http://api.openweathermap.org/data/2.5/weather?\"\\ncity = \"London,uk\"  # Replace with the city you want to get weather for\\ncomplete_url = f\"{base_url}appid={api_key}&q={city}\"\\n\\nresponse = requests.get(complete_url)\\ndata = response.json()\\n\\nif data[\"cod\"] != \"404\":\\n    main = data[\"main\"]\\n    current_temperature = main[\"temp\"] - 273.15  # Convert from Kelvin to Celsius\\n    current_humidity = main[\"humidity\"]\\n    weather = data[\"weather\"][0]\\n    weather_description = weather[\"description\"]\\n    print(f\"Current Temperature: {current_temperature:.2f}°C\\\\nWeather Description: {weather_description}\")\\nelse:\\n    print(\"City not found.\")\\n```\\n\\nMake sure to sign up for an API key on OpenWeatherMap first (https://openweathermap.org/api). Then replace the `your_openweathermap_api_key` and `city` variables with your own API key and desired city, respectively. This example uses London, UK as a default city.\\n\\nThis code will print the current temperature in Celsius and weather description for the specified city. Keep in mind that you\\'ll need to have Python installed on your system to run this code.'},\n",
       " {'role': 'user',\n",
       "  'content': 'what code can you give me that is useful for the awpy2 package '},\n",
       " {'role': 'assistant',\n",
       "  'content': ' The `awpy2` package allows you to access APIs from various sources, including OpenWeatherMap. Here\\'s an example of using `awpy2` to get the current weather:\\n\\n```python\\nimport awpy2 as awp\\n\\napi_key = \"your_openweathermap_api_key\"  # Replace with your actual API key\\ncity = \"London,uk\"  # Replace with the city you want to get weather for\\n\\nowm = awp.OpenWeatherMap(api_key)\\ndata = owm.weather(city=city)\\n\\nif data[\"cod\"] != 404:\\n    main = data[\"main\"]\\n    current_temperature = main[\"temp\"] - 273.15  # Convert from Kelvin to Celsius\\n    current_humidity = main[\"humidity\"]\\n    weather = data[\"weather\"][0]\\n    weather_description = weather[\"description\"]\\n    print(f\"Current Temperature: {current_temperature:.2f}°C\\\\nWeather Description: {weather_description}\")\\nelse:\\n    print(\"City not found.\")\\n```\\n\\nMake sure to install the awpy2 package using `pip install awpy2`. Then replace the `your_openweathermap_api_key` and `city` variables with your own API key and desired city, respectively. This example uses London, UK as a default city.\\n\\nThis code will print the current temperature in Celsius and weather description for the specified city. Keep in mind that you\\'ll need to have Python installed on your system and the awpy2 package to run this code.'},\n",
       " {'role': 'user', 'content': 'end'},\n",
       " {'role': 'assistant',\n",
       "  'content': ' I hope this helps! Let me know if you have any more questions or need further assistance. If you find my answers helpful, feel free to upvote them. Have a great day! :)'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  French is the main language spoken in Paris, but you will also find speakers of other languages due to immigration and international influence.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Example conversation history\n",
    "conversation_history = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm good! How can I help you?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many people live there?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Paris has around 2.1 million people.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the main language spoken there?\"}  # Last user input\n",
    "]\n",
    "\n",
    "# Find the last user message\n",
    "last_user_message = None\n",
    "for message in reversed(conversation_history):  # Start from the last message\n",
    "    if message[\"role\"] == \"user\":\n",
    "        last_user_message = message\n",
    "        break\n",
    "\n",
    "# Ensure there is a user message to send\n",
    "if last_user_message:\n",
    "    # Exclude the last AI response to avoid confusion\n",
    "    messages_to_send = [msg for msg in conversation_history if msg[\"role\"] == \"user\" or msg[\"role\"] == \"assistant\"]\n",
    "    \n",
    "    # Send request to Ollama proxy\n",
    "    response = requests.post(\n",
    "        \"https://ollama-proxy.cent-su.org/api/chat\",\n",
    "        json={\"model\": \"mistral\", \"messages\": messages_to_send},\n",
    "        stream=True  # Enable streaming response\n",
    "    )\n",
    "\n",
    "    # Process response\n",
    "    full_response = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            json_obj = json.loads(line.decode(\"utf-8\"))\n",
    "            if \"message\" in json_obj and \"content\" in json_obj[\"message\"]:\n",
    "                full_response += json_obj[\"message\"][\"content\"]\n",
    "\n",
    "    print(\"AI:\", full_response)\n",
    "else:\n",
    "    print(\"No valid user input found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**You:** write me code that tells me what the weather is"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n",
       "\n",
       "<span class=\"n\">api_key</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;your_openweathermap_api_key&quot;</span>  <span class=\"c1\"># Replace with your actual API key</span>\n",
       "<span class=\"n\">base_url</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;http://api.openweathermap.org/data/2.5/weather?&quot;</span>\n",
       "<span class=\"n\">city</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;London,uk&quot;</span>  <span class=\"c1\"># Replace with the city you want to get weather for</span>\n",
       "<span class=\"n\">complete_url</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">base_url</span><span class=\"si\">}</span><span class=\"s2\">appid=</span><span class=\"si\">{</span><span class=\"n\">api_key</span><span class=\"si\">}</span><span class=\"s2\">&amp;q=</span><span class=\"si\">{</span><span class=\"n\">city</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span>\n",
       "\n",
       "<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">complete_url</span><span class=\"p\">)</span>\n",
       "<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">json</span><span class=\"p\">()</span>\n",
       "\n",
       "<span class=\"k\">if</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">&quot;cod&quot;</span><span class=\"p\">]</span> <span class=\"o\">!=</span> <span class=\"s2\">&quot;404&quot;</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">main</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">&quot;main&quot;</span><span class=\"p\">]</span>\n",
       "    <span class=\"n\">current_temperature</span> <span class=\"o\">=</span> <span class=\"n\">main</span><span class=\"p\">[</span><span class=\"s2\">&quot;temp&quot;</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"mf\">273.15</span>  <span class=\"c1\"># Convert from Kelvin to Celsius</span>\n",
       "    <span class=\"n\">current_humidity</span> <span class=\"o\">=</span> <span class=\"n\">main</span><span class=\"p\">[</span><span class=\"s2\">&quot;humidity&quot;</span><span class=\"p\">]</span>\n",
       "    <span class=\"n\">weather</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">&quot;weather&quot;</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
       "    <span class=\"n\">weather_description</span> <span class=\"o\">=</span> <span class=\"n\">weather</span><span class=\"p\">[</span><span class=\"s2\">&quot;description&quot;</span><span class=\"p\">]</span>\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Current Temperature: </span><span class=\"si\">{</span><span class=\"n\">current_temperature</span><span class=\"si\">:</span><span class=\"s2\">.2f</span><span class=\"si\">}</span><span class=\"s2\">°C</span><span class=\"se\">\\n</span><span class=\"s2\">Weather Description: </span><span class=\"si\">{</span><span class=\"n\">weather_description</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">else</span><span class=\"p\">:</span>\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;City not found.&quot;</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{requests}\n",
       "\n",
       "\\PY{n}{api\\PYZus{}key} \\PY{o}{=} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{your\\PYZus{}openweathermap\\PYZus{}api\\PYZus{}key}\\PY{l+s+s2}{\\PYZdq{}}  \\PY{c+c1}{\\PYZsh{} Replace with your actual API key}\n",
       "\\PY{n}{base\\PYZus{}url} \\PY{o}{=} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{http://api.openweathermap.org/data/2.5/weather?}\\PY{l+s+s2}{\\PYZdq{}}\n",
       "\\PY{n}{city} \\PY{o}{=} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{London,uk}\\PY{l+s+s2}{\\PYZdq{}}  \\PY{c+c1}{\\PYZsh{} Replace with the city you want to get weather for}\n",
       "\\PY{n}{complete\\PYZus{}url} \\PY{o}{=} \\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{base\\PYZus{}url}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{appid=}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{api\\PYZus{}key}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZam{}q=}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{city}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\n",
       "\n",
       "\\PY{n}{response} \\PY{o}{=} \\PY{n}{requests}\\PY{o}{.}\\PY{n}{get}\\PY{p}{(}\\PY{n}{complete\\PYZus{}url}\\PY{p}{)}\n",
       "\\PY{n}{data} \\PY{o}{=} \\PY{n}{response}\\PY{o}{.}\\PY{n}{json}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "\\PY{k}{if} \\PY{n}{data}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cod}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]} \\PY{o}{!=} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{404}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:}\n",
       "    \\PY{n}{main} \\PY{o}{=} \\PY{n}{data}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{main}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\n",
       "    \\PY{n}{current\\PYZus{}temperature} \\PY{o}{=} \\PY{n}{main}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{temp}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]} \\PY{o}{\\PYZhy{}} \\PY{l+m+mf}{273.15}  \\PY{c+c1}{\\PYZsh{} Convert from Kelvin to Celsius}\n",
       "    \\PY{n}{current\\PYZus{}humidity} \\PY{o}{=} \\PY{n}{main}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{humidity}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\n",
       "    \\PY{n}{weather} \\PY{o}{=} \\PY{n}{data}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{weather}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\n",
       "    \\PY{n}{weather\\PYZus{}description} \\PY{o}{=} \\PY{n}{weather}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{description}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Current Temperature: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{current\\PYZus{}temperature}\\PY{l+s+si}{:}\\PY{l+s+s2}{.2f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{°C}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{Weather Description: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{weather\\PYZus{}description}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\\PY{k}{else}\\PY{p}{:}\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{City not found.}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "import requests\n",
       "\n",
       "api_key = \"your_openweathermap_api_key\"  # Replace with your actual API key\n",
       "base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
       "city = \"London,uk\"  # Replace with the city you want to get weather for\n",
       "complete_url = f\"{base_url}appid={api_key}&q={city}\"\n",
       "\n",
       "response = requests.get(complete_url)\n",
       "data = response.json()\n",
       "\n",
       "if data[\"cod\"] != \"404\":\n",
       "    main = data[\"main\"]\n",
       "    current_temperature = main[\"temp\"] - 273.15  # Convert from Kelvin to Celsius\n",
       "    current_humidity = main[\"humidity\"]\n",
       "    weather = data[\"weather\"][0]\n",
       "    weather_description = weather[\"description\"]\n",
       "    print(f\"Current Temperature: {current_temperature:.2f}°C\\nWeather Description: {weather_description}\")\n",
       "else:\n",
       "    print(\"City not found.\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**You:** what code can you give me that is useful for the awpy2 package "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">awpy2</span> <span class=\"k\">as</span> <span class=\"nn\">awp</span>\n",
       "\n",
       "<span class=\"n\">api_key</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;your_openweathermap_api_key&quot;</span>  <span class=\"c1\"># Replace with your actual API key</span>\n",
       "<span class=\"n\">city</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;London,uk&quot;</span>  <span class=\"c1\"># Replace with the city you want to get weather for</span>\n",
       "\n",
       "<span class=\"n\">owm</span> <span class=\"o\">=</span> <span class=\"n\">awp</span><span class=\"o\">.</span><span class=\"n\">OpenWeatherMap</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"p\">)</span>\n",
       "<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">owm</span><span class=\"o\">.</span><span class=\"n\">weather</span><span class=\"p\">(</span><span class=\"n\">city</span><span class=\"o\">=</span><span class=\"n\">city</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"k\">if</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">&quot;cod&quot;</span><span class=\"p\">]</span> <span class=\"o\">!=</span> <span class=\"mi\">404</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">main</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">&quot;main&quot;</span><span class=\"p\">]</span>\n",
       "    <span class=\"n\">current_temperature</span> <span class=\"o\">=</span> <span class=\"n\">main</span><span class=\"p\">[</span><span class=\"s2\">&quot;temp&quot;</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"mf\">273.15</span>  <span class=\"c1\"># Convert from Kelvin to Celsius</span>\n",
       "    <span class=\"n\">current_humidity</span> <span class=\"o\">=</span> <span class=\"n\">main</span><span class=\"p\">[</span><span class=\"s2\">&quot;humidity&quot;</span><span class=\"p\">]</span>\n",
       "    <span class=\"n\">weather</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">&quot;weather&quot;</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
       "    <span class=\"n\">weather_description</span> <span class=\"o\">=</span> <span class=\"n\">weather</span><span class=\"p\">[</span><span class=\"s2\">&quot;description&quot;</span><span class=\"p\">]</span>\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Current Temperature: </span><span class=\"si\">{</span><span class=\"n\">current_temperature</span><span class=\"si\">:</span><span class=\"s2\">.2f</span><span class=\"si\">}</span><span class=\"s2\">°C</span><span class=\"se\">\\n</span><span class=\"s2\">Weather Description: </span><span class=\"si\">{</span><span class=\"n\">weather_description</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">else</span><span class=\"p\">:</span>\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;City not found.&quot;</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{awpy2} \\PY{k}{as} \\PY{n+nn}{awp}\n",
       "\n",
       "\\PY{n}{api\\PYZus{}key} \\PY{o}{=} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{your\\PYZus{}openweathermap\\PYZus{}api\\PYZus{}key}\\PY{l+s+s2}{\\PYZdq{}}  \\PY{c+c1}{\\PYZsh{} Replace with your actual API key}\n",
       "\\PY{n}{city} \\PY{o}{=} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{London,uk}\\PY{l+s+s2}{\\PYZdq{}}  \\PY{c+c1}{\\PYZsh{} Replace with the city you want to get weather for}\n",
       "\n",
       "\\PY{n}{owm} \\PY{o}{=} \\PY{n}{awp}\\PY{o}{.}\\PY{n}{OpenWeatherMap}\\PY{p}{(}\\PY{n}{api\\PYZus{}key}\\PY{p}{)}\n",
       "\\PY{n}{data} \\PY{o}{=} \\PY{n}{owm}\\PY{o}{.}\\PY{n}{weather}\\PY{p}{(}\\PY{n}{city}\\PY{o}{=}\\PY{n}{city}\\PY{p}{)}\n",
       "\n",
       "\\PY{k}{if} \\PY{n}{data}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cod}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]} \\PY{o}{!=} \\PY{l+m+mi}{404}\\PY{p}{:}\n",
       "    \\PY{n}{main} \\PY{o}{=} \\PY{n}{data}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{main}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\n",
       "    \\PY{n}{current\\PYZus{}temperature} \\PY{o}{=} \\PY{n}{main}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{temp}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]} \\PY{o}{\\PYZhy{}} \\PY{l+m+mf}{273.15}  \\PY{c+c1}{\\PYZsh{} Convert from Kelvin to Celsius}\n",
       "    \\PY{n}{current\\PYZus{}humidity} \\PY{o}{=} \\PY{n}{main}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{humidity}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\n",
       "    \\PY{n}{weather} \\PY{o}{=} \\PY{n}{data}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{weather}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\n",
       "    \\PY{n}{weather\\PYZus{}description} \\PY{o}{=} \\PY{n}{weather}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{description}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Current Temperature: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{current\\PYZus{}temperature}\\PY{l+s+si}{:}\\PY{l+s+s2}{.2f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{°C}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{Weather Description: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{weather\\PYZus{}description}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\\PY{k}{else}\\PY{p}{:}\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{City not found.}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "import awpy2 as awp\n",
       "\n",
       "api_key = \"your_openweathermap_api_key\"  # Replace with your actual API key\n",
       "city = \"London,uk\"  # Replace with the city you want to get weather for\n",
       "\n",
       "owm = awp.OpenWeatherMap(api_key)\n",
       "data = owm.weather(city=city)\n",
       "\n",
       "if data[\"cod\"] != 404:\n",
       "    main = data[\"main\"]\n",
       "    current_temperature = main[\"temp\"] - 273.15  # Convert from Kelvin to Celsius\n",
       "    current_humidity = main[\"humidity\"]\n",
       "    weather = data[\"weather\"][0]\n",
       "    weather_description = weather[\"description\"]\n",
       "    print(f\"Current Temperature: {current_temperature:.2f}°C\\nWeather Description: {weather_description}\")\n",
       "else:\n",
       "    print(\"City not found.\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**You:** end  \n",
       "**AI:**  I hope this helps! Let me know if you have any more questions or need further assistance. If you find my answers helpful, feel free to upvote them. Have a great day! :)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from IPython.display import display, Markdown, Code\n",
    "\n",
    "# API endpoint for Ollama proxy\n",
    "OLLAMA_URL = \"https://ollama-proxy.cent-su.org/api/chat\"\n",
    "\n",
    "# Conversation history\n",
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Exit condition\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Ending chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Add user message to history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Send request to Ollama proxy\n",
    "    response = requests.post(\n",
    "        OLLAMA_URL,\n",
    "        json={\"model\": \"mistral\", \"messages\": conversation_history},\n",
    "        stream=True  # Enable streaming response\n",
    "    )\n",
    "\n",
    "    # Process response\n",
    "    full_response = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            json_obj = json.loads(line.decode(\"utf-8\"))\n",
    "            if \"message\" in json_obj and \"content\" in json_obj[\"message\"]:\n",
    "                full_response += json_obj[\"message\"][\"content\"]\n",
    "\n",
    "    # Print AI response in the terminal\n",
    "    # print(\"AI:\", full_response)\n",
    "\n",
    "    # Add AI response to conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "    # **Format Output for Jupyter Notebook**\n",
    "    if \"```python\" in full_response:  # If AI response includes a Python code block\n",
    "        code_block = full_response.split(\"```python\")[1].split(\"```\")[0]  # Extract code\n",
    "        display(Markdown(f\"**You:** {user_input}\"))  # Display user input as Markdown\n",
    "        display(Code(code_block, language=\"python\"))  # Display code in a proper code cell\n",
    "    else:\n",
    "        display(Markdown(f\"**You:** {user_input}  \\n**AI:** {full_response}\"))  # Markdown for normal text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Here's a Python program that allows users to input an equation in the form of ax + b = c and solves for 'x':\n",
      "\n",
      "```python\n",
      "def solve_for_x():\n",
      "    # Get coefficients from user\n",
      "    try:\n",
      "        a = float(input(\"Enter coefficient a: \"))\n",
      "        b = float(input(\"Enter coefficient b: \"))\n",
      "        c = float(input(\"Enter constant term c: \"))\n",
      "    except ValueError:\n",
      "        print(\"Invalid input. Please enter a number.\")\n",
      "        return\n",
      "\n",
      "    # Solve for x using the equation ax + b = c\n",
      "    try:\n",
      "        x = (c - b) / a\n",
      "        print(f\"The value of x is {x:.2f}\")\n",
      "    except ZeroDivisionError:\n",
      "        print(\"Error: cannot divide by zero.\")\n",
      "\n",
      "def main():\n",
      "    while True:\n",
      "        solve_for_x()\n",
      "        response = input(\"Do you want to solve for another variable? (y/n): \")\n",
      "        if response.lower() != 'y':\n",
      "            break\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "In this code, the `solve_for_x` function prompts the user for coefficients a, b and c. It then solves for x using the equation ax + b = c and prints out the result.\n",
      "\n",
      "The program uses a `while True` loop to repeatedly ask the user if they want to solve another variable until they choose not to.\n",
      "\n",
      "Please note that this code assumes that the input is in the form of linear equations (ax + b = c), where 'a', 'b' and 'c' are numbers.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**You:** write me code to solve algebra problems with finding x"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">solve_for_x</span><span class=\"p\">():</span>\n",
       "    <span class=\"c1\"># Get coefficients from user</span>\n",
       "    <span class=\"k\">try</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">(</span><span class=\"s2\">&quot;Enter coefficient a: &quot;</span><span class=\"p\">))</span>\n",
       "        <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">(</span><span class=\"s2\">&quot;Enter coefficient b: &quot;</span><span class=\"p\">))</span>\n",
       "        <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">(</span><span class=\"s2\">&quot;Enter constant term c: &quot;</span><span class=\"p\">))</span>\n",
       "    <span class=\"k\">except</span> <span class=\"ne\">ValueError</span><span class=\"p\">:</span>\n",
       "        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Invalid input. Please enter a number.&quot;</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span>\n",
       "\n",
       "    <span class=\"c1\"># Solve for x using the equation ax + b = c</span>\n",
       "    <span class=\"k\">try</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">c</span> <span class=\"o\">-</span> <span class=\"n\">b</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">a</span>\n",
       "        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;The value of x is </span><span class=\"si\">{</span><span class=\"n\">x</span><span class=\"si\">:</span><span class=\"s2\">.2f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">except</span> <span class=\"ne\">ZeroDivisionError</span><span class=\"p\">:</span>\n",
       "        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Error: cannot divide by zero.&quot;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">():</span>\n",
       "    <span class=\"k\">while</span> <span class=\"kc\">True</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">solve_for_x</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"nb\">input</span><span class=\"p\">(</span><span class=\"s2\">&quot;Do you want to solve for another variable? (y/n): &quot;</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span> <span class=\"o\">!=</span> <span class=\"s1\">&#39;y&#39;</span><span class=\"p\">:</span>\n",
       "            <span class=\"k\">break</span>\n",
       "\n",
       "<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">main</span><span class=\"p\">()</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k}{def} \\PY{n+nf}{solve\\PYZus{}for\\PYZus{}x}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{c+c1}{\\PYZsh{} Get coefficients from user}\n",
       "    \\PY{k}{try}\\PY{p}{:}\n",
       "        \\PY{n}{a} \\PY{o}{=} \\PY{n+nb}{float}\\PY{p}{(}\\PY{n+nb}{input}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Enter coefficient a: }\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{)}\n",
       "        \\PY{n}{b} \\PY{o}{=} \\PY{n+nb}{float}\\PY{p}{(}\\PY{n+nb}{input}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Enter coefficient b: }\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{)}\n",
       "        \\PY{n}{c} \\PY{o}{=} \\PY{n+nb}{float}\\PY{p}{(}\\PY{n+nb}{input}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Enter constant term c: }\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{)}\n",
       "    \\PY{k}{except} \\PY{n+ne}{ValueError}\\PY{p}{:}\n",
       "        \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Invalid input. Please enter a number.}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \\PY{k}{return}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Solve for x using the equation ax + b = c}\n",
       "    \\PY{k}{try}\\PY{p}{:}\n",
       "        \\PY{n}{x} \\PY{o}{=} \\PY{p}{(}\\PY{n}{c} \\PY{o}{\\PYZhy{}} \\PY{n}{b}\\PY{p}{)} \\PY{o}{/} \\PY{n}{a}\n",
       "        \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{The value of x is }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{x}\\PY{l+s+si}{:}\\PY{l+s+s2}{.2f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{k}{except} \\PY{n+ne}{ZeroDivisionError}\\PY{p}{:}\n",
       "        \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Error: cannot divide by zero.}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{main}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{while} \\PY{k+kc}{True}\\PY{p}{:}\n",
       "        \\PY{n}{solve\\PYZus{}for\\PYZus{}x}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{response} \\PY{o}{=} \\PY{n+nb}{input}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Do you want to solve for another variable? (y/n): }\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \\PY{k}{if} \\PY{n}{response}\\PY{o}{.}\\PY{n}{lower}\\PY{p}{(}\\PY{p}{)} \\PY{o}{!=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{y}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:}\n",
       "            \\PY{k}{break}\n",
       "\n",
       "\\PY{k}{if} \\PY{n+nv+vm}{\\PYZus{}\\PYZus{}name\\PYZus{}\\PYZus{}} \\PY{o}{==} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{\\PYZus{}\\PYZus{}main\\PYZus{}\\PYZus{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:}\n",
       "    \\PY{n}{main}\\PY{p}{(}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "def solve_for_x():\n",
       "    # Get coefficients from user\n",
       "    try:\n",
       "        a = float(input(\"Enter coefficient a: \"))\n",
       "        b = float(input(\"Enter coefficient b: \"))\n",
       "        c = float(input(\"Enter constant term c: \"))\n",
       "    except ValueError:\n",
       "        print(\"Invalid input. Please enter a number.\")\n",
       "        return\n",
       "\n",
       "    # Solve for x using the equation ax + b = c\n",
       "    try:\n",
       "        x = (c - b) / a\n",
       "        print(f\"The value of x is {x:.2f}\")\n",
       "    except ZeroDivisionError:\n",
       "        print(\"Error: cannot divide by zero.\")\n",
       "\n",
       "def main():\n",
       "    while True:\n",
       "        solve_for_x()\n",
       "        response = input(\"Do you want to solve for another variable? (y/n): \")\n",
       "        if response.lower() != 'y':\n",
       "            break\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from IPython.display import display, Markdown, Code\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Choose model\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Exit condition\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Ending chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Add user input to conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Generate response using Ollama\n",
    "    response = ollama.chat(model='llama3.2', messages=conversation_history)\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    full_response = response[\"message\"][\"content\"]\n",
    "\n",
    "    # Print response in the terminal\n",
    "    print(\"AI:\", full_response)\n",
    "\n",
    "    # Add AI response to conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "    # **Format Output for Jupyter Notebook**\n",
    "    if \"```python\" in full_response:  # If AI response includes a Python code block\n",
    "        code_block = full_response.split(\"```python\")[1].split(\"```\")[0]  # Extract code\n",
    "        display(Markdown(f\"**You:** {user_input}\"))  # Display user input as Markdown\n",
    "        display(Code(code_block, language=\"python\"))  # Display code in a proper code cell\n",
    "    else:\n",
    "        display(Markdown(f\"**You:** {user_input}  \\n**AI:** {full_response}\"))  # Markdown for normal text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x is 1.33\n",
      "Invalid input. Please enter a number.\n"
     ]
    }
   ],
   "source": [
    "def solve_for_x():\n",
    "    # Get coefficients from user\n",
    "    try:\n",
    "        a = float(input(\"Enter coefficient a: \"))\n",
    "        b = float(input(\"Enter coefficient b: \"))\n",
    "        c = float(input(\"Enter constant term c: \"))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "        return\n",
    "\n",
    "    # Solve for x using the equation ax + b = c\n",
    "    try:\n",
    "        x = (c - b) / a\n",
    "        print(f\"The value of x is {x:.2f}\")\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Error: cannot divide by zero.\")\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        solve_for_x()\n",
    "        response = input(\"Do you want to solve for another variable? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistral\"  # You can switch to \"llama2\" or another model\n",
    "import ollama\n",
    "from IPython.display import display, Markdown, Code\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Choose model\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Exit condition\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Ending chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Add user input to conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Generate response using Ollama\n",
    "    response = ollama.chat(model=model_name, messages=conversation_history)\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    full_response = response[\"message\"][\"content\"]\n",
    "\n",
    "    # Print response in the terminal\n",
    "    print(\"AI:\", full_response)\n",
    "\n",
    "    # Add AI response to conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "    # **Format Output for Jupyter Notebook**\n",
    "    if \"```python\" in full_response:  # If AI response includes a Python code block\n",
    "        code_block = full_response.split(\"```python\")[1].split(\"```\")[0]  # Extract code\n",
    "        display(Markdown(f\"**You:** {user_input}\"))  # Display user input as Markdown\n",
    "        display(Code(code_block, language=\"python\"))  # Display code in a proper code cell\n",
    "    else:\n",
    "        display(Markdown(f\"**You:** {user_input}  \\n**AI:** {full_response}\"))  # Markdown for normal text\n",
    "\n",
    "\n",
    "#This model returned an incorrect code cell for \"give me code that can solve an algebra problem that finds x\"\n",
    "#Returned error:\n",
    "# AttributeError                            Traceback (most recent call last)\n",
    "# Cell In[5], line 12\n",
    "#      10 coefficient_a = 2\n",
    "#      11 constant_b = -3\n",
    "# ---> 12 x = solve_linear_equation(coefficient_a, constant_b)\n",
    "#      13 print(\"The solution for the linear equation is:\", x)\n",
    "\n",
    "# Cell In[5], line 6\n",
    "#       4 x = sp.Symbol('x')\n",
    "#       5 eq = a * x + b\n",
    "# ----> 6 sol = eq.solve()\n",
    "#       7 return sol\n",
    "\n",
    "# AttributeError: 'Add' object has no attribute 'solve'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Add' object has no attribute 'solve'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m coefficient_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     11\u001b[0m constant_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m---> 12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43msolve_linear_equation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoefficient_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstant_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe solution for the linear equation is:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x)\n",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m, in \u001b[0;36msolve_linear_equation\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mSymbol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m eq \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m----> 6\u001b[0m sol \u001b[38;5;241m=\u001b[39m \u001b[43meq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sol\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Add' object has no attribute 'solve'"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "def solve_linear_equation(a, b):\n",
    "    x = sp.Symbol('x')\n",
    "    eq = a * x + b\n",
    "    sol = eq.solve()\n",
    "    return sol\n",
    "\n",
    "# Example usage:\n",
    "coefficient_a = 2\n",
    "constant_b = -3\n",
    "x = solve_linear_equation(coefficient_a, constant_b)\n",
    "print(\"The solution for the linear equation is:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model requires more system memory (8.4 GiB) than is available (6.0 GiB) (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m conversation_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_input})\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Generate response using Ollama\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversation_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Extract the assistant's reply\u001b[39;00m\n\u001b[0;32m     26\u001b[0m full_response \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_client.py:333\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_client.py:122\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[0;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResponseError\u001b[0m: model requires more system memory (8.4 GiB) than is available (6.0 GiB) (status code: 500)"
     ]
    }
   ],
   "source": [
    "model_name = \"llama2\"  # You can switch to \"llama2\" or another model\n",
    "import ollama\n",
    "from IPython.display import display, Markdown, Code\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Choose model\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Exit condition\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Ending chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Add user input to conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Generate response using Ollama\n",
    "    response = ollama.chat(model=model_name, messages=conversation_history)\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    full_response = response[\"message\"][\"content\"]\n",
    "\n",
    "    # Print response in the terminal\n",
    "    print(\"AI:\", full_response)\n",
    "\n",
    "    # Add AI response to conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "    # **Format Output for Jupyter Notebook**\n",
    "    if \"```python\" in full_response:  # If AI response includes a Python code block\n",
    "        code_block = full_response.split(\"```python\")[1].split(\"```\")[0]  # Extract code\n",
    "        display(Markdown(f\"**You:** {user_input}\"))  # Display user input as Markdown\n",
    "        display(Code(code_block, language=\"python\"))  # Display code in a proper code cell\n",
    "    else:\n",
    "        display(Markdown(f\"**You:** {user_input}  \\n**AI:** {full_response}\"))  # Markdown for normal text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "gpt-4.5-preview\n",
      "gpt-4.5-preview-2025-02-27\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "dall-e-3\n",
      "dall-e-2\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview\n",
      "o1-mini-2024-09-12\n",
      "o1-preview-2024-09-12\n",
      "o1-mini\n",
      "o1-preview\n",
      "gpt-4o-mini-audio-preview\n",
      "whisper-1\n",
      "gpt-4o\n",
      "omni-moderation-latest\n",
      "gpt-4o-2024-05-13\n",
      "omni-moderation-2024-09-26\n",
      "babbage-002\n",
      "tts-1-hd-1106\n",
      "text-embedding-3-large\n",
      "gpt-4o-2024-11-20\n",
      "tts-1-hd\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini\n",
      "tts-1\n",
      "tts-1-1106\n",
      "davinci-002\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-16k\n",
      "text-embedding-3-small\n",
      "text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "client = openai.OpenAI(api_key=API_KEY)\n",
    "\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    print(\"Available models:\")\n",
    "    for model in models:\n",
    "        print(model.id)\n",
    "except openai.OpenAIError as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m conversation_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_input})\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Generate response using OpenAI\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversation_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduce response length\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Extract the assistant's reply\u001b[39;00m\n\u001b[0;32m     37\u001b[0m full_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:879\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    876\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    877\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    878\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1290\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1278\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1285\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1286\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1287\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1288\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1289\u001b[0m     )\n\u001b[1;32m-> 1290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:967\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1056\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1055\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1056\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1105\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1056\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1055\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1056\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1105\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\awrm7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1071\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1068\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1070\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1071\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1074\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1075\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1079\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1080\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from IPython.display import display, Markdown, Code\n",
    "\n",
    "# API Key (Replace with your actual OpenAI API key)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=API_KEY, base_url='https://api.openai.com/v1')\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Choose the best available model\n",
    "model_name = \"gpt-4o\"\n",
    "  # Best model from your available models\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Exit condition\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Ending chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Add user input to conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Generate response using OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=conversation_history,\n",
    "        max_tokens=200  # Reduce response length\n",
    "    )\n",
    "\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    full_response = response.choices[0].message.content\n",
    "\n",
    "    # Print response in the terminal\n",
    "    print(\"AI:\", full_response)\n",
    "\n",
    "    # Add AI response to conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "    # **Format Output for Jupyter Notebook**\n",
    "    if \"```python\" in full_response:  # If AI response includes a Python code block\n",
    "        code_block = full_response.split(\"```python\")[1].split(\"```\")[0]  # Extract codez\n",
    "        display(Markdown(f\"**You:** {user_input}\"))  # Display user input as Markdown\n",
    "        display(Code(code_block, language=\"python\"))  # Display code in a proper code cell\n",
    "    else:\n",
    "        display(Markdown(f\"**You:** {user_input}  \\n**AI:** {full_response}\"))  # Markdown for normal text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
